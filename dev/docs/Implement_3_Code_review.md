This script provides a structured way to ensure the LLM's output is not just functional but also well-engineered according to established best practices. Remember, critical human oversight is key.

**How to Use:**

1.  **Start with Phase 1:** Get a clear picture of what the LLM was asked to do and what it produced.
2.  **Move to Phase 2:** Systematically go through the diagnostic questions, using your Phase 1 understanding to make them specific to the code you're reviewing.
3.  **Document Findings:** Note down any issues, concerns, or areas needing improvement.

**LLM Code Review Script**

**Phase 1: Understand the Context & LLM's Output**

*   **1.1. The Prompt & Requirements:**
    *   What was the exact prompt given to the LLM?
    *   What were the explicit functional and non-functional requirements (e.g., performance, security, specific technologies)?
    *   What constraints or existing codebase context was the LLM provided with?

*   **1.2. LLM's Solution - Core Functionality:**
    *   What problem does the generated code *actually* solve or attempt to solve?
    *   What are the main inputs and expected outputs of this code?
    *   Briefly, what is the high-level approach or algorithm the LLM implemented?

*   **1.3. LLM's Solution - Key Components & Technologies:**
    *   What core libraries, frameworks, or significant third-party packages did the LLM choose/use?
        *   *For `[each key package]`, what is its role?*
    *   Did the LLM generate significant custom logic, components, or utility functions?
        *   *For `[each major custom piece]`, what is its specific purpose?*
    *   What assumptions does the LLM's code appear to make (e.g., about data structures, environment, other modules)?

*   **1.4. LLM's Solution - Data Flow & Interactions:**
    *   How does data flow through the generated code?
    *   Does it interact with external services, APIs, or data stores? If so, which ones and how?

**Phase 2: Diagnose Potential Pitfalls in the LLM's Code**

*(Based on your understanding from Phase 1, ask these specific questions about the generated `[code/component/module]`)*

*   **2.1. Adherence to Prompt & Requirements:**
    *   Does the code fully address all aspects of the original prompt and stated requirements?
    *   Are there any deviations or misunderstandings of the requirements?

*   **2.2. Correctness & Logic:**
    *   Is the core logic sound and free of obvious errors?
    *   Does it handle common use cases correctly?

*   **2.3. Reinventing the Wheel:**
    *   Is `[specific custom logic/utility generated by LLM]` recreating functionality already available in `[a specified/standard library/framework]` that *should* have been used?

*   **2.4. Redundant or Conflicting Tools:**
    *   Is the LLM using multiple tools/libraries (e.g., `[Tool A]` and `[Tool B]`) for `[a single task like routing]` where one would suffice, or where they might conflict?

*   **2.5. Obsolete/Legacy/Non-Idiomatic Code:**
    *   Is `[a specific package/pattern used by LLM]` outdated, deprecated, or not the current idiomatic way to solve `[the problem]` within `[the target ecosystem/framework]`?

*   **2.6. Over-Engineering / Unnecessary Complexity:**
    *   Is the complexity of `[the LLM's solution/component]` proportionate to the problem, or could a significantly simpler approach meet the requirements?
    *   Does it introduce abstractions or layers without clear benefit for this specific problem?

*   **2.7. Error Handling & Resilience:**
    *   How does the code handle potential errors, invalid inputs, or failures (e.g., in `[API calls]`, `[data processing]`)? Is it robust?
    *   Are edge cases considered and handled?

*   **2.8. Performance:**
    *   Are there any obvious performance bottlenecks in `[the LLM's code]` (e.g., inefficient loops, excessive operations in `[a critical path]`)?
    *   If performance was a requirement, does it seem likely to meet it?

*   **2.9. Security:**
    *   If `[handling user input]` or `[interacting with external systems]`, are there potential security vulnerabilities (e.g., XSS, injection, insecure data handling, hardcoded secrets)?
    *   Are inputs validated and outputs sanitized appropriately?

*   **2.10. Testability:**
    *   Is `[the generated code/module]` structured in a way that is easy to unit test? Are there tight couplings or excessive side effects making testing difficult?

*   **2.11. Maintainability & Readability:**
    *   Is the code clear, well-structured, and easy for another developer to understand?
    *   Are naming conventions sensible? Is there adequate (but not excessive) commenting where logic is complex?
    *   Does it follow project-specific coding standards (if provided to LLM)?

*   **2.12. Scalability (if relevant):**
    *   If `[the feature/system]` needed to handle significantly more `[load/data]`, would `[the LLM's design]` scale effectively?

*   **2.13. Configuration:**
    *   If the code involves configuration, is it handled cleanly? Are there hardcoded values that should be configurable?

*   **2.14. Dependencies:**
    *   Did the LLM introduce new dependencies? Are they appropriate, well-maintained, and secure?